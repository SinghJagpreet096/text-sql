GPT_CONFIG_124M = {
    "vocab_size" : 50257, # vocabulary size
    "ctx_len" : 1024,     # context length
    "emb_dim" : 768,      # embedding dimesension
    "n_head" : 12,        # number of attention heads
    "n_layers" : 12,      # number of layers
    "drop_rate" : 0.1,    # Dropout rate
    "qkv_bias" : False
}