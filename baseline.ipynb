{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             question\n",
      "0   department_management How many heads of the de...\n",
      "1   department_management List the name, born stat...\n",
      "2   department_management List the creation year, ...\n",
      "3   department_management What are the maximum and...\n",
      "4   department_management What is the average numb...\n",
      "5   department_management What are the names of th...\n",
      "6   department_management What are the distinct cr...\n",
      "7   department_management What are the names of th...\n",
      "8   department_management In which year were most ...\n",
      "9   department_management Show the name and number...\n",
      "10  department_management How many acting statuses...\n",
      "                                                query\n",
      "0          SELECT count(*) FROM head WHERE age  >  56\n",
      "1   SELECT name ,  born_state ,  age FROM head ORD...\n",
      "2   SELECT creation ,  name ,  budget_in_billions ...\n",
      "3   SELECT max(budget_in_billions) ,  min(budget_i...\n",
      "4   SELECT avg(num_employees) FROM department WHER...\n",
      "5   SELECT name FROM head WHERE born_state != 'Cal...\n",
      "6   SELECT DISTINCT T1.creation FROM department AS...\n",
      "7   SELECT born_state FROM head GROUP BY born_stat...\n",
      "8   SELECT creation FROM department GROUP BY creat...\n",
      "9   SELECT T1.name ,  T1.num_employees FROM depart...\n",
      "10  SELECT count(DISTINCT temporary_acting) FROM m...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path):\n",
    "    selected_columns = [\"db_id\",\"question\",\"query\"]\n",
    "    df = pd.read_parquet(file_path,columns=selected_columns)\n",
    "    X_train = df[[\"db_id\",\"question\"]]\n",
    "    y_train = df[[\"query\"]]\n",
    "    return X_train,y_train\n",
    "\n",
    "X_train, y_train = load_data(\"data/train/0000.parquet\")\n",
    "\n",
    "X_train['question'] = X_train[\"db_id\"] +\" \"+ X_train['question']\n",
    "\n",
    "X_train = X_train.drop('db_id',axis=1)\n",
    "\n",
    "print(X_train.loc[:10])\n",
    "print(y_train.loc[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_padded: [[  48  249   19   21  987    2    1  226    6  403   22 2579    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249   24    1    9 1065  174    3   91    2    1  987    2  226\n",
      "    82   27   91    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249   24    1 1302   73    9    3  330    2   23   48    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    4    6    1   86    3  144  330    2    1  226    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    4   10    1   30   14    2   72    2    1  226   42  152\n",
      "    10  209  237    3  774    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    4    6    1    8    2    1  987   18    6 1065 1826    1\n",
      "  1066  174    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    4    6    1   61 1302  238    2    1  226 1067   27   31\n",
      "   841 1065    7  174 2580    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    4    6    1    8    2    1  272  223   60   65   74  987\n",
      "   145 1065    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249    7   26   73  145   28  226 2581    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  48  249   20    1    9    3   14    2   72   15    1  226 1067   27\n",
      "   987   42 2582 1655  379   10 1827    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "y_train_padded: [[   4   12    3  480    7   41 2200    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4    8  948   85   41    3  480   14   11   41    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4 1145    8  151   30 1406    3   40    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4   46  151   30 1406   76  151   30 1406    3   40    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4   27   78   70    3   40    7 1146  166  179   23  643    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4    8    3  480    7  948   85 1124    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4   22    1 1145    3   40    5    1    9  853    5    2   10    1\n",
      "    40    6    2   40    6    9  480    5   13   10    2  480    6   13\n",
      "   480    6    7   13  948   85 2201    0    0    0    0    0]\n",
      " [   4  948   85    3  480   15   11  948   85   31   12   91    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4 1145    3   40   15   11 1145   14   11   12   20   19   18    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4    1    8    1   78   70    3   40    5    1    9  853    5    2\n",
      "    10    1   40    6    2   40    6    7    2 1407 1408  481    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "vocab_size_input: 3261\n",
      "vocab_size_output: 2405\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize input sequences (questions)\n",
    "tokenizer_input = Tokenizer()\n",
    "tokenizer_input.fit_on_texts(X_train['question'])\n",
    "X_train_sequences = tokenizer_input.texts_to_sequences(X_train['question'])\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding='post')\n",
    "\n",
    "print(f\"X_train_padded: {X_train_padded[:10]}\")\n",
    "\n",
    "# Tokenize target sequences (SQL queries)\n",
    "tokenizer_output = Tokenizer()\n",
    "tokenizer_output.fit_on_texts(y_train['query'])\n",
    "y_train_sequences = tokenizer_output.texts_to_sequences(y_train['query'])\n",
    "y_train_padded = pad_sequences(y_train_sequences, padding='post',maxlen=X_train_padded.shape[1])\n",
    "\n",
    "print(f\"y_train_padded: {y_train_padded[:10]}\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "\n",
    "print(f\"vocab_size_input: {vocab_size_input}\")\n",
    "print(f\"vocab_size_output: {vocab_size_output}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size_input, output_dim=embedding_dim, input_length=X_train_padded.shape[1]),\n",
    "    LSTM(units, return_sequences=True),\n",
    "    Dense(units=vocab_size_output, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 2.0169 - accuracy: 0.6063 - val_loss: 3.0323 - val_accuracy: 0.5604\n",
      "Epoch 2/30\n",
      "175/175 [==============================] - 14s 80ms/step - loss: 1.9692 - accuracy: 0.6079 - val_loss: 3.0370 - val_accuracy: 0.5751\n",
      "Epoch 3/30\n",
      "175/175 [==============================] - 14s 80ms/step - loss: 1.9086 - accuracy: 0.6113 - val_loss: 3.0856 - val_accuracy: 0.5727\n",
      "Epoch 4/30\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 1.8548 - accuracy: 0.6141 - val_loss: 3.1282 - val_accuracy: 0.5720\n",
      "Epoch 5/30\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 1.7939 - accuracy: 0.6173 - val_loss: 3.1973 - val_accuracy: 0.5739\n",
      "Epoch 6/30\n",
      "175/175 [==============================] - 15s 84ms/step - loss: 1.7385 - accuracy: 0.6209 - val_loss: 3.2179 - val_accuracy: 0.5554\n",
      "Epoch 7/30\n",
      "175/175 [==============================] - 14s 80ms/step - loss: 1.6888 - accuracy: 0.6242 - val_loss: 3.2062 - val_accuracy: 0.5686\n",
      "Epoch 8/30\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 1.6367 - accuracy: 0.6273 - val_loss: 3.3157 - val_accuracy: 0.5797\n",
      "Epoch 9/30\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 1.5877 - accuracy: 0.6306 - val_loss: 3.3217 - val_accuracy: 0.5664\n",
      "Epoch 10/30\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 1.5429 - accuracy: 0.6343 - val_loss: 3.2991 - val_accuracy: 0.5730\n",
      "Epoch 11/30\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 1.4988 - accuracy: 0.6382 - val_loss: 3.4154 - val_accuracy: 0.5676\n",
      "Epoch 12/30\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 1.4573 - accuracy: 0.6425 - val_loss: 3.4542 - val_accuracy: 0.5627\n",
      "Epoch 13/30\n",
      "175/175 [==============================] - 15s 85ms/step - loss: 1.4165 - accuracy: 0.6467 - val_loss: 3.4612 - val_accuracy: 0.5648\n",
      "Epoch 14/30\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 1.3803 - accuracy: 0.6506 - val_loss: 3.5615 - val_accuracy: 0.5637\n",
      "Epoch 15/30\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 1.3505 - accuracy: 0.6536 - val_loss: 3.5380 - val_accuracy: 0.5633\n",
      "Epoch 16/30\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 1.3197 - accuracy: 0.6578 - val_loss: 3.5703 - val_accuracy: 0.5534\n",
      "Epoch 17/30\n",
      "175/175 [==============================] - 220s 1s/step - loss: 1.2959 - accuracy: 0.6611 - val_loss: 3.6097 - val_accuracy: 0.5646\n",
      "Epoch 18/30\n",
      "175/175 [==============================] - 15s 87ms/step - loss: 1.2734 - accuracy: 0.6640 - val_loss: 3.6279 - val_accuracy: 0.5593\n",
      "Epoch 19/30\n",
      "175/175 [==============================] - 15s 87ms/step - loss: 1.2420 - accuracy: 0.6692 - val_loss: 3.6700 - val_accuracy: 0.5616\n",
      "Epoch 20/30\n",
      "175/175 [==============================] - 15s 85ms/step - loss: 1.2211 - accuracy: 0.6724 - val_loss: 3.6959 - val_accuracy: 0.5596\n",
      "Epoch 21/30\n",
      "175/175 [==============================] - 15s 86ms/step - loss: 1.1941 - accuracy: 0.6772 - val_loss: 3.7372 - val_accuracy: 0.5620\n",
      "Epoch 22/30\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 1.1736 - accuracy: 0.6811 - val_loss: 3.7631 - val_accuracy: 0.5500\n",
      "Epoch 23/30\n",
      "175/175 [==============================] - 15s 83ms/step - loss: 1.1620 - accuracy: 0.6831 - val_loss: 3.8078 - val_accuracy: 0.5616\n",
      "Epoch 24/30\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 1.1347 - accuracy: 0.6884 - val_loss: 3.8336 - val_accuracy: 0.5586\n",
      "Epoch 25/30\n",
      "175/175 [==============================] - 15s 88ms/step - loss: 1.1118 - accuracy: 0.6931 - val_loss: 3.8155 - val_accuracy: 0.5572\n",
      "Epoch 26/30\n",
      "175/175 [==============================] - 15s 83ms/step - loss: 1.0910 - accuracy: 0.6975 - val_loss: 3.9166 - val_accuracy: 0.5536\n",
      "Epoch 27/30\n",
      "175/175 [==============================] - 15s 83ms/step - loss: 1.0755 - accuracy: 0.7006 - val_loss: 3.8937 - val_accuracy: 0.5520\n",
      "Epoch 28/30\n",
      "175/175 [==============================] - 16s 89ms/step - loss: 1.0569 - accuracy: 0.7044 - val_loss: 3.9430 - val_accuracy: 0.5515\n",
      "Epoch 29/30\n",
      "175/175 [==============================] - 15s 87ms/step - loss: 1.0660 - accuracy: 0.7022 - val_loss: 3.9165 - val_accuracy: 0.5551\n",
      "Epoch 30/30\n",
      "175/175 [==============================] - 17s 94ms/step - loss: 1.0363 - accuracy: 0.7097 - val_loss: 4.0139 - val_accuracy: 0.5606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x298227fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train_padded, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted SQL Query: select count from temporary where 56\n"
     ]
    }
   ],
   "source": [
    "def generate_sql_sequence(question):\n",
    "    question_sequence = tokenizer_input.texts_to_sequences([question])\n",
    "    question_padded = pad_sequences(question_sequence, padding='post', maxlen=X_train_padded.shape[1])\n",
    "    predicted_sequence = model.predict(question_padded)[0]\n",
    "    predicted_sql_query = [tokenizer_output.index_word[idx] for idx in np.argmax(predicted_sequence, axis=-1) if idx != 0]\n",
    "    return \" \".join(predicted_sql_query)\n",
    "\n",
    "# Example usage:\n",
    "new_question = \"department_management How many heads of the departments ?\"\n",
    "predicted_sql_query = generate_sql_sequence(new_question)\n",
    "print(\"Predicted SQL Query:\", predicted_sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"artifacts/baseline.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
